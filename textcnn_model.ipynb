{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(sizeOfVoc,embedding_dims,LenOfSeq,class_num):\n",
    "    input = Input(shape=(LenOfSeq,))\n",
    "    embedding = Embedding(sizeOfVoc, embedding_dims, input_length=LenOfSeq)(input)\n",
    "    convs = []\n",
    "    for kernel_size in [3, 4, 5]:\n",
    "        c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        convs.append(c)\n",
    "    x = Concatenate()(convs)\n",
    "\n",
    "    output = Dense(class_num, activation='softmax')(x)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "stopwords =[ \"'d\",\"'ll\",\"'m\",\"'re\",\"'s\",\"'t\",\"'ve\",\"ZT\",\"ZZ\",\"a\",\"a's\",\"able\",\"about\",\"above\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"adopted\",\"affected\",\"affecting\",\"affects\",\"after\",\"afterwards\",\"again\",\"against\",\"ah\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"announce\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"apparently\",\"appear\",\"appreciate\",\"appropriate\",\"approximately\",\"are\",\"area\",\"areas\",\"aren\",\"aren't\",\"arent\",\"arise\",\"around\",\"as\",\"aside\",\"ask\",\"asked\",\"asking\",\"asks\",\"associated\",\"at\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"backed\",\"backing\",\"backs\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"began\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"being\",\"beings\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"big\",\"biol\",\"both\",\"brief\",\"briefly\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"ca\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"case\",\"cases\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clear\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"couldnt\",\"course\",\"currently\",\"d\",\"date\",\"definitely\",\"describe\",\"described\",\"despite\",\"did\",\"didn't\",\"differ\",\"different\",\"differently\",\"discuss\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downed\",\"downing\",\"downs\",\"downwards\",\"due\",\"during\",\"e\",\"each\",\"early\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ended\",\"ending\",\"ends\",\"enough\",\"entirely\",\"especially\",\"et\",\"et-al\",\"etc\",\"even\",\"evenly\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"face\",\"faces\",\"fact\",\"facts\",\"far\",\"felt\",\"few\",\"ff\",\"fifth\",\"find\",\"finds\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"from\",\"full\",\"fully\",\"further\",\"furthered\",\"furthering\",\"furthermore\",\"furthers\",\"g\",\"gave\",\"general\",\"generally\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"going\",\"gone\",\"good\",\"goods\",\"got\",\"gotten\",\"great\",\"greater\",\"greatest\",\"greetings\",\"group\",\"grouped\",\"grouping\",\"groups\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hed\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hers\",\"herself\",\"hes\",\"hi\",\"hid\",\"high\",\"higher\",\"highest\",\"him\",\"himself\",\"his\",\"hither\",\"home\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"hundred\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"id\",\"ie\",\"if\",\"ignored\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"in\",\"inasmuch\",\"inc\",\"include\",\"indeed\",\"index\",\"indicate\",\"indicated\",\"indicates\",\"information\",\"inner\",\"insofar\",\"instead\",\"interest\",\"interested\",\"interesting\",\"interests\",\"into\",\"invention\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"itd\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"keys\",\"kg\",\"kind\",\"km\",\"knew\",\"know\",\"known\",\"knows\",\"l\",\"large\",\"largely\",\"last\",\"lately\",\"later\",\"latest\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"long\",\"longer\",\"longest\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"made\",\"mainly\",\"make\",\"makes\",\"making\",\"man\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"member\",\"members\",\"men\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"more\",\"moreover\",\"most\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"my\",\"myself\",\"n\",\"n't\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needed\",\"needing\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"newer\",\"newest\",\"next\",\"nine\",\"ninety\",\"no\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"nor\",\"normally\",\"nos\",\"not\",\"noted\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"number\",\"numbers\",\"o\",\"obtain\",\"obtained\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"older\",\"oldest\",\"omitted\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"open\",\"opened\",\"opening\",\"opens\",\"or\",\"ord\",\"order\",\"ordered\",\"ordering\",\"orders\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"owing\",\"own\",\"p\",\"page\",\"pages\",\"part\",\"parted\",\"particular\",\"particularly\",\"parting\",\"parts\",\"past\",\"per\",\"perhaps\",\"place\",\"placed\",\"places\",\"please\",\"plus\",\"point\",\"pointed\",\"pointing\",\"points\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"presented\",\"presenting\",\"presents\",\"presumably\",\"previously\",\"primarily\",\"probably\",\"problem\",\"problems\",\"promptly\",\"proud\",\"provides\",\"put\",\"puts\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"re\",\"readily\",\"really\",\"reasonably\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"room\",\"rooms\",\"run\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"second\",\"secondly\",\"seconds\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"sees\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"she'll\",\"shed\",\"shes\",\"should\",\"shouldn't\",\"show\",\"showed\",\"showing\",\"shown\",\"showns\",\"shows\",\"side\",\"sides\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"small\",\"smaller\",\"smallest\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"state\",\"states\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"such\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that'll\",\"that's\",\"that've\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there'll\",\"there's\",\"there've\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"theyd\",\"theyre\",\"thing\",\"things\",\"think\",\"thinks\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"thou\",\"though\",\"thoughh\",\"thought\",\"thoughts\",\"thousand\",\"three\",\"throug\",\"through\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"to\",\"today\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"turn\",\"turned\",\"turning\",\"turns\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wanted\",\"wanting\",\"wants\",\"was\",\"wasn't\",\"way\",\"ways\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"wed\",\"welcome\",\"well\",\"wells\",\"went\",\"were\",\"weren't\",\"what\",\"what'll\",\"what's\",\"whatever\",\"whats\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whim\",\"whither\",\"who\",\"who'll\",\"who's\",\"whod\",\"whoever\",\"whole\",\"whom\",\"whomever\",\"whos\",\"whose\",\"why\",\"widely\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"words\",\"work\",\"worked\",\"working\",\"works\",\"world\",\"would\",\"wouldn't\",\"www\",\"x\",\"y\",\"year\",\"years\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"youd\",\"young\",\"younger\",\"youngest\",\"your\",\"youre\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\",\"zt\",\"zz\"]\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    if(string.startswith('>')):\n",
    "        return ''\n",
    "    if(string.startswith(' +')|string.startswith(' |')):\n",
    "        return ''\n",
    "    reg = \"[^0-9A-Za-z\\u4e00-\\u9fa5]\"\n",
    "    string = re.sub(reg, \" \", string)\n",
    "    reg =  \"[\\d]\"\n",
    "    string = re.sub(reg, \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def tokenize(path):\n",
    "    f = open(path, \"r\", encoding='UTF-8')\n",
    "    alllines =list(f.readlines())\n",
    "    f.close()\n",
    "    x_token = [clean_str(sent)  for sent in alllines ]\n",
    "    #x_token = [clean_str(n) for a in x_token for n in a ]\n",
    "    x_token = [term.split(' ')  for term in x_token ]\n",
    "    x_token = [term for a in x_token for term in a if ((term!='')&(term not in stopwords)) ]\n",
    "    return x_token\n",
    "def cod(seq,voc):\n",
    "    cod_seq=[]\n",
    "    for term in seq:\n",
    "        if term in voc:\n",
    "            cod_seq.append(voc[term])\n",
    "    return cod_seq\n",
    "\n",
    "def get_voc(tokensList,max_freq_term):\n",
    "    voc = dict()\n",
    "    i=0\n",
    "    voc1 = {term for seq in tokensList for term in seq}\n",
    "    for term in voc1:\n",
    "        if term in voc :\n",
    "            voc[term]+=1\n",
    "        else:\n",
    "            i+=1\n",
    "            d1 ={term:i}\n",
    "            voc.update(d1) \n",
    "    list1 = sorted(voc.items(),key=lambda x:x[1])\n",
    "    items = list1[0:max_freq_term]\n",
    "    term2index = dict()\n",
    "    index2term=dict()\n",
    "    i=0\n",
    "    for item in items:\n",
    "        i+=1\n",
    "        d1 ={item[0]:i}\n",
    "        term2index.update(d1) \n",
    "        d2 = {i:item[0]}\n",
    "        index2term.update(d2) \n",
    "    d3 ={'<pad>':0}\n",
    "    term2index.update(d3) \n",
    "    d4 ={0:'<pad>'}\n",
    "    index2term.update(d4) \n",
    "    return items,term2index,index2term\n",
    "\n",
    "def coder(file_list1,file_list2):\n",
    "    file_list = file_list1+file_list2\n",
    "    tokensList = [tokenize(path) for path in file_list]\n",
    "    items,term2index,index2term = get_voc(tokensList,3000)\n",
    "    labels1 = [0] * len(file_list1)\n",
    "    labels2 = [1] * len(file_list2)\n",
    "    labels = labels1+labels2\n",
    "    code_text_list = [cod(seq,term2index) for seq in tokensList ]\n",
    "    return items,term2index,index2term,code_text_list,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = '/opt/data_txt/1'\n",
    "p2 = '/opt/data_txt/2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_names = os.listdir(p1)\n",
    "file_list1 = [os.path.join(p1,file) for file in file_names]\n",
    "file_names = os.listdir(p2)\n",
    "file_list2 = [os.path.join(p2,file) for file in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "items,term2index,index2term,code_text_list,lables = coder(file_list1,file_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3001"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_val(code_text_list,lables,r_train,r_test,r_val):\n",
    "    np.random.seed(113)\n",
    "    indices = np.arange(len(code_text_list))\n",
    "    np.random.shuffle(indices)\n",
    "    b=np.array(code_text_list)\n",
    "    xs = b[indices]\n",
    "    b=np.array(lables)\n",
    "    labels = b[indices]   \n",
    "    length = len(xs)\n",
    "    l_tr = int(length*r_train)\n",
    "    l_te = int(length*r_test)\n",
    "    l_va = int(length*r_val)    \n",
    "    x_train, y_train = np.array(xs[:l_tr]), np.array(labels[:l_tr])\n",
    "    x_test, y_test = np.array(xs[l_tr:l_tr+l_te]), np.array(labels[l_tr:l_tr+l_te])\n",
    "    x_val, y_val = np.array(xs[l_tr+l_te:]), np.array(labels[l_tr+l_te:])\n",
    "    return (x_train, y_train), (x_test, y_test),(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf1_15/lib/python3.6/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test),(x_val, y_val) = generate_train_test_val(code_text_list,lables,0.6,0.2,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(len(term2index),256,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(x_train,value=0,padding='post',maxlen=100)\n",
    "x_test_pad = tf.keras.preprocessing.sequence.pad_sequences(x_test,value=0,padding='post',maxlen=100)\n",
    "x_val_pad = tf.keras.preprocessing.sequence.pad_sequences(x_val,value=0,padding='post',maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "404/404 [==============================] - 2s 4ms/sample - loss: 0.6731 - acc: 0.6337\n",
      "Epoch 2/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.5234 - acc: 0.8985\n",
      "Epoch 3/40\n",
      "404/404 [==============================] - 1s 4ms/sample - loss: 0.3879 - acc: 0.9777\n",
      "Epoch 4/40\n",
      "404/404 [==============================] - 1s 4ms/sample - loss: 0.2561 - acc: 0.9876\n",
      "Epoch 5/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.1491 - acc: 0.9926\n",
      "Epoch 6/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0782 - acc: 0.9950\n",
      "Epoch 7/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0398 - acc: 1.0000\n",
      "Epoch 8/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0209 - acc: 1.0000\n",
      "Epoch 9/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 10/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 11/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 12/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 13/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 14/40\n",
      "404/404 [==============================] - 1s 4ms/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 15/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 16/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 17/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 18/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 19/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 20/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 21/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 22/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 9.1767e-04 - acc: 1.0000\n",
      "Epoch 23/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 8.4166e-04 - acc: 1.0000\n",
      "Epoch 24/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 7.7432e-04 - acc: 1.0000\n",
      "Epoch 25/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 7.1622e-04 - acc: 1.0000\n",
      "Epoch 26/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 6.6679e-04 - acc: 1.0000\n",
      "Epoch 27/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 6.1881e-04 - acc: 1.0000\n",
      "Epoch 28/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 5.7817e-04 - acc: 1.0000\n",
      "Epoch 29/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 5.4037e-04 - acc: 1.0000\n",
      "Epoch 30/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 5.0510e-04 - acc: 1.0000\n",
      "Epoch 31/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 4.7300e-04 - acc: 1.0000\n",
      "Epoch 32/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 4.4492e-04 - acc: 1.0000\n",
      "Epoch 33/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 4.1870e-04 - acc: 1.0000\n",
      "Epoch 34/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 3.9514e-04 - acc: 1.0000\n",
      "Epoch 35/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 3.7384e-04 - acc: 1.0000\n",
      "Epoch 36/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 3.5442e-04 - acc: 1.0000\n",
      "Epoch 37/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 3.3670e-04 - acc: 1.0000\n",
      "Epoch 38/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 3.1966e-04 - acc: 1.0000\n",
      "Epoch 39/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 3.0434e-04 - acc: 1.0000\n",
      "Epoch 40/40\n",
      "404/404 [==============================] - 1s 3ms/sample - loss: 2.9031e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_binary = to_categorical(y_train)\n",
    "history = model.fit(x_train_pad,y_binary, epochs=40,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 1ms/sample - loss: 0.2251 - acc: 0.8955\n",
      "[0.2251213864604039, 0.8955224]\n"
     ]
    }
   ],
   "source": [
    "y_binary_test = to_categorical(y_test)\n",
    "results = model.evaluate(x_test_pad, y_binary_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    }
   ],
   "source": [
    "model.save('textcnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wwarf', 'silver', 'ucs', 'indiana', 'wayne', 'warf', 'subject', 'atf', 'burns', 'dividian', 'ranch', 'update', 'nntp', 'posting', 'host', 'silver', 'ucs', 'indiana', 'organization', 'indiana', 'university', 'distribution', 'usa', 'lines', 'article', 'nate', 'psygate', 'psych', 'indiana', 'nate', 'psygate', 'psych', 'indiana', 'nathan', 'engle', 'writes', 'credible', 'evidence', 'bd', 'set', 'fires', 'atf', 'fbi', 'sayso', 'law', 'enforcements', 'type', 'lie', 'cover', 'ass', 'love', 'share', 'surprising', 'similarity', 'beliefs', 'method', 'funny', 'yeah', 'funny', 'didn', 'wait', 'fbi', 'spokesdroid', 'reversal', 'proclaiming', 'bd', 'burned', 'death']\n",
      "55\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "f = open('/opt/data_txt/1/54464', \"r\", encoding='UTF-8')\n",
    "alllines =list(f.readlines())\n",
    "f.close()\n",
    "x_token = [clean_str(sent)  for sent in alllines ]\n",
    "#x_token = [clean_str(n) for a in x_token for n in a ]\n",
    "x_token = [term.split(' ')  for term in x_token ]\n",
    "x_token = [term for a in x_token for term in a if ((term!='')&(term not in stopwords)) ]\n",
    "#x_token = [term  for term in x_token if ((term!='')&(term not in stopwords))]\n",
    "print(x_token)\n",
    "voc = dict()\n",
    "i=0\n",
    "voc1 = {term for seq in tokensList for term in seq}\n",
    "for term in voc1:\n",
    "    if voc[term] is not None:\n",
    "        voc[term]+=1\n",
    "    else:\n",
    "        d1 ={term:i}\n",
    "        i+=1\n",
    "        voc.update(d1) \n",
    "list1= sorted(voc.items(),key=lambda x:x[1])\n",
    "voc,code_text_list = coder([x_token])\n",
    "print(len(voc))\n",
    "print(len(code_text_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1_15",
   "language": "python",
   "name": "tf1_15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
